{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5a9XIzOkaMxDhGgZ7EC8r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParsaRouzrokh/PyTorch_Brain_Tumor_Segmentation_2D/blob/main/Brain_Tumor_Segmentation_GUI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Brain Tumor Segmentation Model Graphical User Interface!**\n",
        "\n",
        "*Author: Parsa Rouzrokh, MD, MPH*\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In this notebook, I have created a front-end for one of the brain tumor models that we previously trained. By utilizing Hugging Face's Gradio library, the user interface of our model is now accessible simply by running the following notebook.\n",
        "\n",
        "You can check out the final version of the interface [here](https://huggingface.co/spaces/Parsa00r/Brain_Seg) without having to run the notebook or install any additional packages!\n",
        "\n",
        "\n",
        "*   The required packages for running this notebook are available in the requirements.txt file.\n",
        "\n",
        "*   The information provided by users is being recorded in a publicly accessible notebook called \"patients_info.xlsx\" which is available on my Google Drive. If you would like access to this file or would like to evaluate the recording process, feel free to make a copy of [\"Patients_Info.xlsx\"](https://docs.google.com/spreadsheets/d/1uiXs3Ml_gT4d3VxlLiWsOWY6nE6ZLeMF/edit#gid=592561244) into your own Drive and use it *(Ensure about the name of the file \"Patients_Info.xlsx\")*.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Table of Contents:\n",
        "\n",
        "*   Intro 1: Installing Libraries\n",
        "*   Intro 2: Importing Necessary Modules\n",
        "*   Intro 3: Mounting Google Drive\n",
        "*   Part 1: Model Collection\n",
        "*   Part 2: Required Files/Folders Collection\n",
        "*   Part 3: User Interface\n",
        "    - Part 3.1: Function 1: Img_Hash\n",
        "    - Part 3.2: Function 2: U_Flex_Predict\n",
        "    - Part 3.3: Function 3: Submission\n",
        "    - Part 3.4: Launching Our Demo\n",
        "\n"
      ],
      "metadata": {
        "id": "mptLxrxc_0Wr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Intro 1: Installing Libraries**\n",
        "We start by installing required libraries for this project:\n",
        "\n",
        "\n",
        "*   gradio (ver 3.45.0)\n",
        "*   monai\n",
        "*   gdown"
      ],
      "metadata": {
        "id": "Ork1hMHnJbef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio==3.45.0\n",
        "!pip install monai\n",
        "!pip install gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBkXIF0xKCB2",
        "outputId": "b1ff137d-3d11-4164-8316-5f8ffed50851"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio==3.45.0\n",
            "  Downloading gradio-3.45.0-py3-none-any.whl (20.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio==3.45.0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.45.0) (4.2.2)\n",
            "Collecting fastapi (from gradio==3.45.0)\n",
            "  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio==3.45.0)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.5.2 (from gradio==3.45.0)\n",
            "  Downloading gradio_client-0.5.2-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.3/298.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio==3.45.0)\n",
            "  Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.14.0 (from gradio==3.45.0)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==3.45.0) (6.1.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.45.0) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.45.0) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.45.0) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.45.0) (1.23.5)\n",
            "Collecting orjson~=3.0 (from gradio==3.45.0)\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio==3.45.0) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.45.0) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.45.0) (9.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio==3.45.0) (1.10.13)\n",
            "Collecting pydub (from gradio==3.45.0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio==3.45.0)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.45.0) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.45.0) (2.31.0)\n",
            "Collecting semantic-version~=2.0 (from gradio==3.45.0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.45.0) (4.5.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio==3.45.0)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio==3.45.0)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.5.2->gradio==3.45.0) (2023.6.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.45.0) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.45.0) (4.19.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.45.0) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio==3.45.0) (3.12.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio==3.45.0) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.45.0) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.45.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.45.0) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.45.0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.45.0) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.45.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==3.45.0) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.45.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.45.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.45.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.45.0) (2023.7.22)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio==3.45.0) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio==3.45.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio==3.45.0) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio==3.45.0)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions~=4.0 (from gradio==3.45.0)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Collecting httpcore<0.19.0,>=0.18.0 (from httpx->gradio==3.45.0)\n",
            "  Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.45.0) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio==3.45.0) (1.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.45.0) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.45.0) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.45.0) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.45.0) (0.10.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==3.45.0) (1.16.0)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=640dde7faebf4ac38a95e9b741aefe9166d6acb0fa52598b2990edd60fa80860\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, typing-extensions, semantic-version, python-multipart, orjson, h11, aiofiles, uvicorn, starlette, huggingface-hub, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 fastapi-0.104.1 ffmpy-0.3.1 gradio-3.45.0 gradio-client-0.5.2 h11-0.14.0 httpcore-0.18.0 httpx-0.25.0 huggingface-hub-0.18.0 orjson-3.9.10 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.27.0 typing-extensions-4.8.0 uvicorn-0.23.2 websockets-11.0.3\n",
            "Collecting monai\n",
            "  Downloading monai-1.3.0-202310121228-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from monai) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from monai) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (4.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->monai) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->monai) (1.3.0)\n",
            "Installing collected packages: monai\n",
            "Successfully installed monai-1.3.0\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Intro 2: Importing Necessary Modules**\n",
        "\n",
        "This notebook is using the mentioned standard/third-party modules:\n",
        "\n"
      ],
      "metadata": {
        "id": "sR82wObUJtsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library modules\n",
        "import os\n",
        "import hashlib\n",
        "\n",
        "# Third-party library modules\n",
        "import io\n",
        "import torch\n",
        "import gradio as gr\n",
        "import monai as mn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from PIL import Image\n",
        "import gdown\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# There is no need for \"gpu\" in this demo version!\n",
        "device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "ZSviuhNXy0O7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Intro 3: Mounting Google Drive**\n",
        "\n",
        "For accessing to our local database (\"Patients_Info\"), we should first mount our google drive to our notebook!\n",
        "\n",
        "Please follow these steps to connect your drive to this notebook:\n",
        "\n",
        "*   Step 1: Click on \"Connect to Google Drive\" in the pop-up that appears after running this cell.\n",
        "\n",
        "*   Step 2: Select the Google account into which you have copied the \"Patients_Info.xlsx\" file.\n",
        "\n",
        "*   Step 3: Scroll down in the subsequent window and choose \"Allow\" to grant access to this notebook!\n",
        "\n"
      ],
      "metadata": {
        "id": "it19XZh3Z6ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_-fNej1Zk9Y",
        "outputId": "0274c0fc-6dcd-4d55-a4cb-bc78b322d447"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Model Collection**\n",
        "\n",
        "If you remember, we have trained two models on our MRI 2D slice dataset in the \"Brain_Tumor_segmentation\" notebook:\n",
        "*  UFlex: Using flexible UNets from Monai library\n",
        "*  CustomNet: I wrote it from scratch\n",
        "\n",
        "For creating our interface, I decide to use our UFlex model and weights. Hence, I saved this model using torch.save to my goole drive and made it public for future use (link).\n",
        "\n",
        "So now in this part, let's download this model to our notebook here, and unpack it."
      ],
      "metadata": {
        "id": "pUj3Bvw9AA_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/uc?id=12nY-WdqwHoyUiKVKzpWNjXfuMerTP8Gl'\n",
        "output = 'UFlex_package.pth'\n",
        "gdown.download(url, output, quiet=True)\n",
        "UFlex = torch.load(\"UFlex_package.pth\",map_location=torch.device('cpu'))\n",
        "model = UFlex['U_Flex_architecture']\n",
        "weights = UFlex['U_Flex_state_dict']\n",
        "model.load_state_dict(weights)\n",
        "model.eval()\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "PhZehbm-KVs_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Required Files/Folders Collection**\n",
        "\n",
        "Before moving to the interface developement, there are two more folders we need to load into this notebook. Same as the model, these two folders are also public and accessible using the links below:\n",
        "*  [inference_images](https://drive.usercontent.google.com/download?id=1Cs9ohjiRFa1dYF-8kntn8iatMPmTts9x&export=download&authuser=0&confirm=t&uuid=886c52a3-192e-4d97-a12b-414ff50ddc95&at=APZUnTWwpVgYrOOUn3QBAmvrgaIS:1698916071069)\n",
        "*  [examples](https://drive.usercontent.google.com/download?id=1aAhB5IL5AQh-Rv1Yvn_oioYMmnPYjQJ-&export=download&authuser=0&confirm=t&uuid=cd389704-a47b-45bc-950b-174cf8f171b4&at=APZUnTXcvX2Aj1g2mXXj2GOu4qWN:1698916115402)"
      ],
      "metadata": {
        "id": "KwNSBhswMoid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download(url= \"https://drive.usercontent.google.com/download?id=1Cs9ohjiRFa1dYF-8kntn8iatMPmTts9x&export=download&authuser=0&confirm=t&uuid=886c52a3-192e-4d97-a12b-414ff50ddc95&at=APZUnTWwpVgYrOOUn3QBAmvrgaIS:1698916071069\",\n",
        "               output = \"inference_images.zip\", quiet=False)\n",
        "gdown.download(url= \"https://drive.usercontent.google.com/download?id=1aAhB5IL5AQh-Rv1Yvn_oioYMmnPYjQJ-&export=download&authuser=0&confirm=t&uuid=cd389704-a47b-45bc-950b-174cf8f171b4&at=APZUnTXcvX2Aj1g2mXXj2GOu4qWN:1698916115402\",\n",
        "               output = \"examples.zip\", quiet=False)\n",
        "!unzip -q \"inference_images\" -d inference_images\n",
        "!unzip -q \"examples\" -d examples\n",
        "os.remove(\"inference_images.zip\")\n",
        "os.remove(\"examples.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USDkZzZcOY4r",
        "outputId": "81993841-177d-4fb1-cc5a-734972d6319b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.usercontent.google.com/download?id=1Cs9ohjiRFa1dYF-8kntn8iatMPmTts9x&export=download&authuser=0&confirm=t&uuid=886c52a3-192e-4d97-a12b-414ff50ddc95&at=APZUnTWwpVgYrOOUn3QBAmvrgaIS:1698916071069\n",
            "To: /content/inference_images.zip\n",
            "100%|██████████| 51.2k/51.2k [00:00<00:00, 56.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.usercontent.google.com/download?id=1aAhB5IL5AQh-Rv1Yvn_oioYMmnPYjQJ-&export=download&authuser=0&confirm=t&uuid=cd389704-a47b-45bc-950b-174cf8f171b4&at=APZUnTXcvX2Aj1g2mXXj2GOu4qWN:1698916115402\n",
            "To: /content/examples.zip\n",
            "100%|██████████| 151k/151k [00:00<00:00, 55.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3: User Interface**\n",
        "\n",
        "Gradio is a Python library that allows you to create customizable Graphical User Interface (GUI) components for a machine learning model or any Python function.\n",
        "\n",
        "There are only three required parameters needed to be specified for creating interfaces in gradio:\n",
        "*   The function to create a GUI for\n",
        "*   The desired input components\n",
        "*   The desired output components\n",
        "\n",
        "So first of all, let's design our graphical demo. Our demo should include these parts:\n",
        "\n",
        "\n",
        "\n",
        "1.   Title and description of our model\n",
        "2.   Getting patient's information\n",
        "3.   Getting patient's mri slice\n",
        "4.   Submission of the info and showing the tumor-identified version to user\n",
        "\n",
        "Below, you can see the defined functions before starting the demo cell. But maybe it's better to design your demo first and then, define your functions.\n",
        "\n",
        "For this demo, we define three functions:\n",
        "\n",
        "*   img_hash: A function to convert images to they hash forms for more convenient recording.\n",
        "*   U_Flex_predict: An inference function of our model\n",
        "*   submission: A function responsible for recording all the given info, and returning the output of our model to user.\n",
        "\n",
        "Now, everything is set! The next three sections (Part 3.1, Part 3.2, and Part 3.3), are all about the functions. So feel free to navigate to Part (3.4) for designing part before checking out the functions."
      ],
      "metadata": {
        "id": "yu3QSlrNEaHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3.1: Function 1: Img_Hash\n",
        "\n",
        "Since we want to record the input and output slices, along with the patients' information in an excel file, I decide to convert these arrays to their hash forms.\n"
      ],
      "metadata": {
        "id": "xzDKnW2VEi5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Img_Hash (array):\n",
        "  return hashlib.sha1(array).hexdigest()"
      ],
      "metadata": {
        "id": "YsWJfTydkr-0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3.2: Function 2: U_Flex_Predict\n",
        "\n",
        "We have the best version of our model right now. So it's ready for the inference phase. In this function, we perform some pre/post processings according to the needs of our model, and our demo.   \n"
      ],
      "metadata": {
        "id": "zcco_V3Oks3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def U_Flex_Predict(input_image):\n",
        "  input_image = input_image.convert(\"L\")\n",
        "  image_array =  np.array(input_image)\n",
        "  inference_transforms = mn.transforms.Compose([\n",
        "    mn.transforms.EnsureChannelFirst(channel_dim=\"no_channel\"),\n",
        "    mn.transforms.Resize(spatial_size= image_array.shape),\n",
        "    mn.transforms.ScaleIntensity(minv=0, maxv=1),\n",
        "    mn.transforms.ToTensor()\n",
        "  ])\n",
        "  image = inference_transforms(image_array)\n",
        "  with torch.no_grad():\n",
        "    image = image.unsqueeze(0).to(device)\n",
        "    output = model(image)\n",
        "  infer_img = image.to(device)\n",
        "  infer_out = output.to(device)\n",
        "  plt.imshow(infer_img[0][0].detach().cpu(), cmap='gray')\n",
        "  plt.imshow(infer_out.squeeze(0).argmax(dim=0).detach().cpu(),interpolation='nearest', alpha=0.5)\n",
        "  plt.axis('off')\n",
        "  plot_bytes = io.BytesIO()\n",
        "  plt.savefig(plot_bytes, format='jpg')\n",
        "  plot_bytes.seek(0)\n",
        "  result_image = Image.open(plot_bytes)\n",
        "  result_hash = Img_Hash(np.array(result_image))\n",
        "  input_hash = Img_Hash(np.array(input_image))\n",
        "  result_dict = {\"result_image\": result_image, \"result_hash\": result_hash, \"input_hash\": input_hash}\n",
        "  return result_dict"
      ],
      "metadata": {
        "id": "9fJL36NmKu8r"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3.3: Function 3: Submission\n",
        "In this part, we provide a function which is called after the user clicks on the \"submit\" button. It includes various parts listed below:\n",
        "*   Inputs sanity checks\n",
        "*   Showing the tumor-identified version of input MRI slice\n",
        "*   Recording the patients info, along with input and output hashed images in our local database (Patients_Info.xlsx).\n"
      ],
      "metadata": {
        "id": "L6KVRC8FmnKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Submission(Age,Drug,PMH,Symptoms,Family,Other_Info, Drug_Yes,Family_Yes,Other_Yes,PMH_others, input_image):\n",
        "  necessary_fields = [Age, Drug, PMH, Symptoms, Family, Other_Info]\n",
        "  filled_fields = sum(bool(field) for field in necessary_fields)\n",
        "\n",
        "  # Sanity check part 1:\n",
        "\n",
        "  if Drug == \"Yes\" and not Drug_Yes:\n",
        "    raise gr.Error(\"Please enter the name of your medications before submitting!\")\n",
        "  elif Drug == \"No\" and Drug_Yes:\n",
        "    raise gr.Error(\"Please leave the 'Medications or Treatments' field blank if you have no history of taking medications!\")\n",
        "  if \"Others\" in PMH and not PMH_others:\n",
        "    raise gr.Error(\"Please enter the name of your physical conditions which are not listed above before submitting!\")\n",
        "  if \"Others\" not in PMH and PMH_others:\n",
        "    raise gr.Error(\"You have to check 'Others' if you want to add extra conditions\")\n",
        "  elif \"Nothing\" in PMH and PMH_others:\n",
        "    raise gr.Error(\"Please leave the 'Other Conditions' field blank if you have no other conditions!\")\n",
        "  elif \"Nothing\" in PMH and len(PMH) > 1:\n",
        "    raise gr.Error(\"You can not select 'Nothing' along with other options. Please try again.\")\n",
        "  if Family == \"Yes\" and not Family_Yes:\n",
        "    raise gr.Error(\"Please enter your familiar history before submitting!\")\n",
        "  elif Family == \"No\" and Family_Yes:\n",
        "    raise gr.Error(\"Please leave the 'Familial History' field blank if you have no familial history!\")\n",
        "  if \"No\" in Symptoms and len(Symptoms) > 1:\n",
        "    raise gr.Error(\"You can not select No along with other symptoms!\")\n",
        "  if Other_Info == \"Yes\" and not Other_Yes:\n",
        "    raise gr.Error(\"Please enter any other relevant factors or medical information that\\\n",
        "    you believe may be important for us before submitting!\")\n",
        "    error +=1\n",
        "  elif Other_Info == \"No\" and Other_Yes:\n",
        "    raise gr.Error(\"Please leave the 'Other info' field blank if you have no other info!\")\n",
        "\n",
        "  if not input_image:\n",
        "    raise gr.Error(\"Please upload your brain MRI slice before submission!\")\n",
        "\n",
        "  # sanity check part 2:\n",
        "\n",
        "  sample_1_array = np.array(Image.open(\"examples/sample_image1.jpg\").convert(\"L\"))\n",
        "  sample_2_array = np.array(Image.open(\"examples/sample_image2.jpg\").convert(\"L\"))\n",
        "  sample_3_array = np.array(Image.open(\"examples/sample_image3.jpg\").convert(\"L\"))\n",
        "  sample_1_mask_array = np.array(Image.open(\"examples/sample_image1_mask.jpg\").convert(\"L\"))\n",
        "  sample_2_mask_array = np.array(Image.open(\"examples/sample_image2_mask.jpg\").convert(\"L\"))\n",
        "  sample_3_mask_array = np.array(Image.open(\"examples/sample_image3_mask.jpg\").convert(\"L\"))\n",
        "\n",
        "  if (Img_Hash(sample_1_array) == Img_Hash(np.array(input_image.convert(\"L\"))))\\\n",
        "   or (Img_Hash(sample_2_array) == Img_Hash(np.array(input_image.convert(\"L\"))))\\\n",
        "   or (Img_Hash(sample_3_array) == Img_Hash(np.array(input_image.convert(\"L\"))))\\\n",
        "   or (Img_Hash(sample_1_mask_array) == Img_Hash(np.array(input_image.convert(\"L\"))))\\\n",
        "   or (Img_Hash(sample_2_mask_array) == Img_Hash(np.array(input_image.convert(\"L\"))))\\\n",
        "   or (Img_Hash(sample_3_mask_array) == Img_Hash(np.array(input_image.convert(\"L\")))):\n",
        "       raise gr.Error(\"You can not submit the examples as your MRI slice! Please \\\n",
        "       clear this image and choose your own!\")\n",
        "\n",
        "  # Inferring the input image and recording the info to our local database!\n",
        "\n",
        "  results = U_Flex_Predict(input_image)\n",
        "  if filled_fields == len(necessary_fields) and input_image:\n",
        "      records = [Age,Drug,Drug_Yes,PMH,PMH_others,Symptoms,Family,Family_Yes,\\\n",
        "                 Other_Info,Other_Yes,results['input_hash'],results['result_hash']]\n",
        "      !cp '/content/drive/MyDrive/Patients_Info.xlsx' Patients_Info.xlsx\n",
        "      df = pd.read_excel('Patients_Info.xlsx')\n",
        "      df = df.append(pd.Series(records, index=df.columns), ignore_index=True)\n",
        "      df.to_excel('Patients_Info.xlsx', index=False)\n",
        "      !cp 'Patients_Info.xlsx' '/content/drive/MyDrive/Patients_Info.xlsx'\n",
        "      !rm 'Patients_Info.xlsx'\n",
        "\n",
        "      gr.Info(\"Thank you for your submission! Your information has been received.\\\n",
        "      If you want to do the entire test for another person, click the 'Clear' button and repeat the process.\")\n",
        "\n",
        "      return(results['result_image'])\n",
        "\n",
        "  else:\n",
        "      raise gr.Error(f\" Please fill all the fields before submission\")"
      ],
      "metadata": {
        "id": "YQeXAq6kIThF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3.4: Launching Our Demo**\n",
        "\n",
        "In this part, we initiate our demo and launch it using our functions defined above. Our demo includes:\n",
        "1.   Title and description of our model\n",
        "2.   Creating different elements for making selections, using gardio components\n",
        "3.   Showing some examples and the outputs\n",
        "4.   Creating an image component that can be used to upload images (as an input) or display images (as an output)\n",
        "5.   Creating a submission button and link it with the function defined above\n",
        "\n",
        "After running the following cell, the GUI we have just designed will be ready. You can access it through the provided link below.\n",
        "Please note that the displayed inputs and outputs in the GUI are examples and cannot be used as actual input. However, the \"inference_images\" folder you downloaded earlier contains real 2D FLAIR images that you can use to evaluate the model/demo.\n",
        "Also, you can checkout your local database in your drive and evaluate the recordings!\n",
        "\n"
      ],
      "metadata": {
        "id": "9RzOVSsspgBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "\n",
        "  # 1. Title and description of our model:\n",
        "\n",
        "  gr.Markdown(\"<center><b><font size='10'>Welcome to the Brain Tumor Segmentation Lab!</font></b></center>\")\n",
        "  gr.Markdown(\"<center><b><font size ='5'>In this page, you can upload your 2D \\\n",
        "  Brain FLAIR MRI slice, and detect the tumor location in a blink of an eye!</font></b></center>\")\n",
        "  gr.Markdown(\"<center><font size ='3'>Please follow the instructions in order \\\n",
        "  and provide feedback by contacting me at <a href='mailto:parsa.rouzrokh97@gmail.com'>\\\n",
        "  parsa.rouzrokh97@gmail.com</a> </font></center>\")\n",
        "  gr.Markdown(\"<font size ='2'><b>Step1:</b></font> We need some information \\\n",
        "  about your condition before uploading your MRI slice.\")\n",
        "  gr.Markdown(\"* Rest assured that all your information is securely stored and \\\n",
        "  treated with the utmost confidentiality to ensure your privacy.\")\n",
        "  gr.Markdown(\"* Questions marked with an asterisk (*) are necessary to answer before submission.\")\n",
        "\n",
        "  # 2. Creating different elements for making selections, using gardio components\n",
        "\n",
        "  with gr.Row():\n",
        "      with gr.Column():\n",
        "        Age = gr.Radio(\n",
        "        [\"under 20\", \"20-30\", \"30-40\",\"40-50\",\"50-60\",\"More than 60\"],\n",
        "        label=\"*1.Please tell us your age range:\")\n",
        "        Drug = gr.Radio(\n",
        "          [\"Yes\",\"No\"],\n",
        "          label=\"*2.Are you currently taking any medications or undergoing any \\\n",
        "          other medical treatments?\",\n",
        "          info =\"If you click 'Yes', please tell us the medications or treatments\\\n",
        "          you have in a text box below!\"\n",
        "      )\n",
        "        Drug_Yes = gr.Textbox(placeholder=\"Please fill this if you check 'Yes' above!\",\n",
        "                                label=\"Medications or Treatments:\")\n",
        "        PMH = gr.CheckboxGroup(\n",
        "        [\"Nothing\",\"Hypertension\",\"Diabetes\",\"Seizure\",\"Others\"],\n",
        "        label=\"*3.What are your known medical conditions or history of neurological disorders?\",\n",
        "        info = \"If you click 'others', please tell us the conditions you have in a text box below!\")\n",
        "        PMH_others = gr.Textbox(placeholder=\"Please fill this if you check 'others' above!\",\n",
        "                                label=\"Other Conditions:\")\n",
        "      with gr.Column():\n",
        "        Symptoms = gr.CheckboxGroup(\n",
        "          ['No','persistent headaches','seizures','changes in vision'],\n",
        "          label=\"*4.Have you experienced any recent symptoms related to the brain,\\\n",
        "          such as persistent headaches, seizures, or changes in vision?\")\n",
        "        Family = gr.Radio([\"Yes\",\"No\"],\n",
        "        label=\"*5.Is there a family history of brain\\\n",
        "         tumors or other neurological conditions?\",\n",
        "         info = \"If you click 'Yes', please tell us the history in a text box below!\"\n",
        "         )\n",
        "        Family_Yes = gr.Textbox(placeholder=\"Please fill this if you check 'Yes' above!\",\n",
        "                                label=\"Family History:\")\n",
        "        Other_Info = gr.Radio([\"Yes\",\"No\"],\n",
        "        label=\"*6.Are there any other relevant factors or medical information that \\\n",
        "        you believe may be important for us to know about?\",\n",
        "        info = \"If you click 'Yes', please tell us the details in a text box below!\"\n",
        "         )\n",
        "        Other_Yes = gr.Textbox(placeholder=\"Please fill this if you check 'Yes' above!\",\n",
        "                                label=\"Other Info\")\n",
        "\n",
        "  gr.Markdown(\"<font size ='2'><b>Step2:</b></font> Now it's time to upload the MRI slice!\")\n",
        "  gr.Markdown(\"* Upload your picture by simply clicking on the left box and selecting\\\n",
        "  the MRI image from your local system.\")\n",
        "  gr.Markdown(\"* If you fill all of the blanks above, You will be able to do the submission\\\n",
        "  and get your tumor identified version in the right box!\")\n",
        "  gr.Markdown(\"* We recommend that you crop the identification part of your image\\\n",
        "  by selecting the 'brain volume' using the 'edit' button located at the top right of the uploaded picture.\")\n",
        "  gr.Markdown(\"* For better accuracy, please ensure that you upload your image in\\\n",
        "  the same orientation as the examples we have provided for you below.\")\n",
        "  gr.Markdown(\"<center><font size ='3'> Here are three example MRI 2D slices along\\\n",
        "  with the tumor location identified versions next to them!</font></center>\")\n",
        "\n",
        "  # 3. Showing some examples and the outputs\n",
        "\n",
        "  gr.Gallery([\"examples/sample_image1.jpg\",\n",
        "              \"examples/sample_image1_mask.jpg\",\n",
        "                \"examples/sample_image2.jpg\",\n",
        "                \"examples/sample_image2_mask.jpg\",\n",
        "                \"examples/sample_image3.jpg\",\n",
        "              \"examples/sample_image3_mask.jpg\"],\n",
        "             label = \"Examples:\", object_fit=\"contain\", height=\"auto\",columns=[6], rows=[1])\n",
        "\n",
        "  # 4. Creating an image component that can be used to upload images (as an input) or display images (as an output)\n",
        "\n",
        "  with gr.Row():\n",
        "    with gr.Column():\n",
        "      gr.Markdown(\"Upload your image here!\")\n",
        "      input_image = gr.Image(type='pil')\n",
        "\n",
        "    with gr.Column():\n",
        "      gr.Markdown(\"This is where the output image appears!\")\n",
        "      output_image = gr.Image(type='pil',container=False)\n",
        "\n",
        "  # 5. Creating a submission button and link it with the function defined above\n",
        "\n",
        "  Submit = gr.Button(value=\"Submit!\",interactive=True)\n",
        "  Submit.click(Submission,\n",
        "               inputs=[Age,Drug,PMH,Symptoms,Family,Other_Info,Drug_Yes,Family_Yes,Other_Yes,PMH_others,input_image],\n",
        "               outputs= output_image)\n",
        "  clear = gr.ClearButton([Age,Drug,PMH,Symptoms,Family,Other_Info,Drug_Yes,Family_Yes,Other_Yes,PMH_others,input_image,output_image])\n",
        "  if __name__ == \"__main__\":\n",
        "      demo.queue()\n",
        "      demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "4qYmRncpzOVC",
        "outputId": "cacff94a-cd1d-4eec-865c-8a072bb0de21"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://0090d50af1bb3c296c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0090d50af1bb3c296c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thank you for accompanying me on this journey! I would appreciate it if you could share your feedback with me via email at parsa.rouzrokh97@gmail.com.\n",
        "\n",
        "See you soon!"
      ],
      "metadata": {
        "id": "_RF7GnXAtMkK"
      }
    }
  ]
}